{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion\n",
    "## Contents\n",
    "1. [Loading Data into Google Cloud Storage](#LoadingDataGCS)\n",
    "2. [Loading Data into BigQuery](#LoadingData)<br>\n",
    "    2.1 [From local directory](#localdirectory)<br>\n",
    "    2.2 [From Google Cloud Storage](#loadGCS)<br>\n",
    "    2.3 [Triggering loading when new files are added to Google Cloud Storage](#loadCloudFn)  \n",
    "    2.4 [From Pandas DataFrame](#loadDF)\n",
    "3. [Viewing BigQuery table schema](#ViewSchema)<br> \n",
    "4. [Reading from BigQuery](#ReadingData)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "First, we want to ensure that the `Data.zip` file (and its unzipped contents) are available to this notebooks.<br/>\n",
    "The necessary files should already be uploaded to the `data` directory. If they are not, follow the following instructions.\n",
    "### Step #1: Upload data into your Jupyter notebook\n",
    "Manually download the `Data.zip` file using the upload button. This is necessary because Jupyter does not have access to your local file system.   \n",
    "<img src=\"img/upload_button.png\" title=\"Upload Button\"/>   \n",
    "### Step #2: Unzip the file  \n",
    "Running the following Bash command will unzip the `Data.zip` file and add the contents the `data/` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "unzip Data.zip -d data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='LoadingDataGCS'></a>\n",
    "# 1. Loading Data into Google Cloud Storage (GCS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step #1: Create Google Cloud Storage bucket (Optional)\n",
    "A Google Cloud Storage bucket is...   \n",
    "<br>You only need to create a new GCS bucket if you're not uploading to an existing bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil mb gs://email-propensity-sandbox-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step #2: Upload files to GCS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://data/Data/events.csv [Content-Type=text/csv]...\n",
      "Copying file://data/Data/campaign_types.csv [Content-Type=text/csv]...\n",
      "Copying file://data/Data/Opens.csv [Content-Type=text/csv]...\n",
      "Copying file://data/Data/Sends.csv [Content-Type=text/csv]...\n",
      "Copying file://data/Data/User Info.csv [Content-Type=text/csv]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run\n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "/\n",
      "Operation completed over 5 objects/242.7 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil -m cp data/Data/* gs://email-propensity-sandbox-data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='LoadingData'></a>\n",
    "# 2. Loading Data into BigQuery\n",
    "First, we need to load the sample dataset into BigQuery.   \n",
    "   \n",
    "The same commands can be used from this notebook and from your laptop's command line. The only differences in processes is that you have to authenticate (tell GCP who you are) and authorize (get sufficient permissins) your request when you're loading via laptop command line. When using Cloud Shell or AI Platform Notebooks, your requests are already authenticated and authorized (since your notebook is already running on the cloud!).   \n",
    "<br/>\n",
    "Let's start by identifying the GCP project_id and (desired) BigQuery dataset id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'email-propensity-sandbox'\n",
    "dataset_id = 'test_upload'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='localdirectory'></a>\n",
    "## Option #1: Loading from local directory using Command-line\n",
    "### Step #1: Load data into BigQuery using the bq command-line tool \n",
    "If you want to create your own BigQuery dataset, run the following Bash command to create a new dataset with name `project_id:dataset_id`. You'll get an error message if this dataset already exists.   \n",
    "\n",
    "The bq command-line tool provides a convenient point of entry to interact with the BigQuery service on Google Cloud Platform, although everything you do with bq can be done using the REST API and most things can also be accomplished using the GCP web console. Here, we are asking it to make (mk) a dataset\n",
    "  \n",
    "Datasets in BigQuery function like top-level folders that are used to organize and control access to tables, views, and machine learning models. The dataset is created in the current project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$dataset_id\"\n",
    "bq --location=US mk --dataset $1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following Bash commands to load your local csv into BigQuery.   \n",
    "   \n",
    "In this case, we are asking bq to load the dataset, telling the tool that the source format is CSV and that we would like the tool to auto-detect the schema (i.e., the data types of individual columns).<br/>\n",
    "<br/>\n",
    "The bq tool command uses the following syntax to load local files into BQ:\n",
    "```\n",
    "bq load --autodetect --source_format=CSV [dataset_id].[table_name] [path of local CSV]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r64a13fb39209c49e_0000016c5e2319ff_1 ... (1s) Current status: DONE   "
     ]
    }
   ],
   "source": [
    "%%bash -s \"$dataset_id\"\n",
    "bq load --autodetect --replace --source_format=CSV $1.events data/Data/events.csv\n",
    "bq load --autodetect --replace --source_format=CSV $1.opens data/Data/Opens.csv\n",
    "bq load --autodetect --replace --source_format=CSV $1.sends data/Data/Sends.csv\n",
    "bq load --autodetect --replace --source_format=CSV $1.user_info \"data/Data/User Info.csv\"\n",
    "bq load --autodetect --replace --source_format=CSV $1.campaign_types data/Data/campaign_types.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='loadGCS'></a>\n",
    "## Option #2: Loading from Google Cloud Storage\n",
    "If you want to create your own BigQuery dataset, run the following Bash command to create a new dataset with name `project_id:dataset_id`. You'll get an error message if this dataset already exists.   \n",
    "\n",
    "The bq command-line tool provides a convenient point of entry to interact with the BigQuery service on Google Cloud Platform, although everything you do with bq can be done using the REST API and most things can also be accomplished using the GCP web console. Here, we are asking it to make (mk) a dataset\n",
    "  \n",
    "Datasets in BigQuery function like top-level folders that are used to organize and control access to tables, views, and machine learning models. The dataset is created in the current project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$dataset_id\"\n",
    "bq --location=US mk --dataset $1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bq tool command uses the following syntax to load GCS files into BigQuery:\n",
    "```\n",
    "bq load --autodetect --source_format=CSV [dataset_id].[table_name] [path of CSV in GCS]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$dataset_id\"\n",
    "bq load --autodetect --replace --source_format=CSV $1.events gs://email-propensity-sandbox-data/events.csv\n",
    "bq load --autodetect --replace --source_format=CSV $1.opens  gs://email-propensity-sandbox-data/Opens.csv\n",
    "bq load --autodetect --replace --source_format=CSV $1.sends  gs://email-propensity-sandbox-data/Sends.csv\n",
    "bq load --autodetect --replace --source_format=CSV $1.user_info  \"gs://email-propensity-sandbox-data/User Info.csv\"\n",
    "bq load --autodetect --replace --source_format=CSV $1.campaign_types  gs://email-propensity-sandbox-data/campaign_types.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='loadCloudFn'></a>\n",
    "## Option #3: Triggering loading when new files are added to Google Cloud Storage\n",
    "With Cloud Functions you write simple, single-purpose functions that are attached to events emitted from your cloud infrastructure and services. Your Cloud Function is triggered when an event being watched is fired. Your code executes in a fully managed environment. There is no need to provision any infrastructure or worry about managing any servers.   \n",
    "  \n",
    "This example demonstrates how to trigger a BigQuery load job when a new file is added to a Google Cloud Storage bucket, as shown in the diagram below.  \n",
    "<br/>\n",
    "<img src=\"img/gcs_to_bq.png\" title=\"GCS to BigQuery\" style=\"width: 500px;\"/>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [Cloud Functions UI](http://console.cloud.google.com/functions), select \"Create Function\".  \n",
    "<br>Set the following values:\n",
    "* Trigger: \"Cloud Storage\"\n",
    "* Event Type: \"Finalize/Create\"\n",
    "* Bucket: browse and select bucket where new files will be added\n",
    "* Runtime: Python 3.7<br/>\n",
    "\n",
    "<br/>\n",
    "<b>Paste the below code into the `main.py` inline editor.</b>\n",
    "<br/>\n",
    "\n",
    "```code\n",
    "def import_bigquery(event, context):\n",
    "    \"\"\"Import CSV to BigQuery after file is added to GCS.\n",
    "\n",
    "    Args:\n",
    "         event (dict): Event payload.\n",
    "         context (google.cloud.functions.Context): Metadata for the event.\n",
    "    \"\"\"\n",
    "    from google.cloud import bigquery\n",
    "    \n",
    "    file = event\n",
    "    print(f\"Processing file: {file['name']}.\")\n",
    "    table_id = file['name'].replace('.csv','') # set BQ table name as the filename\n",
    "    bucket = file['bucket']\n",
    "    uri = 'gs://{}/{}'.format(bucket, file['name'])\n",
    "    \n",
    "    client = bigquery.Client()\n",
    "    dataset = client.dataset('test_upload') # replace with your dataset id\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.autodetect = True # auto-detect BigQuery schema\n",
    "    job_config.skip_leading_rows = 1 # skip header row\n",
    "    job_config.source_format = bigquery.SourceFormat.CSV\n",
    "    load_job = client.load_table_from_uri(\n",
    "        uri,\n",
    "        dataset.table(table_id),\n",
    "        job_config=job_config\n",
    "    )\n",
    "```\n",
    "<br/>\n",
    "<b>Paste the below code into the `requirements.txt` inline editor.</b>\n",
    "\n",
    "```\n",
    "google-cloud-bigquery\n",
    "google-cloud-storage\n",
    "```\n",
    "<b>Set \"Function to execute\" as \"import_bigquery\"</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='LoadDF'></a>\n",
    "### Option #4: Load Pandas DataFrame to a BigQuery table \n",
    "You can load a Pandas DataFrame into BigQuery.  \n",
    "<br/>\n",
    "This is important because you'll be able to follow the following process while preprocessing data:\n",
    "1. Query raw data and load results into a Pandas DataFrame.\n",
    "2. Manipulate data using Pandas.  \n",
    "3. Upload DataFrame back to BigQuery.  \n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "  (1, u'What is BigQuery?'),\n",
    "  (2, u'Query essentials'),\n",
    "]\n",
    "df = pd.DataFrame(data, columns=['chapter', 'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()\n",
    "dataset_ref = client.dataset('test_upload') # set to name of dataset\n",
    "table_ref = dataset_ref.table('pandas_table') # set to name of destination table\n",
    "\n",
    "# use load_config to overwrite old table contents\n",
    "load_config = bigquery.job.LoadJobConfig(\n",
    "    create_disposition=bigquery.job.CreateDisposition.CREATE_IF_NEEDED,\n",
    "    write_disposition=bigquery.job.WriteDisposition.WRITE_TRUNCATE)\n",
    "\n",
    "job = client.load_table_from_dataframe(\n",
    "    dataframe=df, # set to name of DataFrame\n",
    "    destination=table_ref,\n",
    "    job_config=load_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ViewSchema'></a>\n",
    "# 3. Viewing BigQuery table schema\n",
    "Before querying a table, it is helpful to first know the table's schema.\n",
    "## Option #1: BigQuery UI\n",
    "Schemas are displaying in the [BigQuery UI](http://console.cloud.google.com/bigquery) under a table's \"Schema\" tab.   \n",
    "\n",
    "## Option #2: Command-Line\n",
    "Use the command-line bq tool to print a json representing a table's metadata. \"name\" corresponds to the column name and \"type\" corresponds to that column's data type.  \n",
    "An example is shown below for the `campaign_types` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"mode\": \"NULLABLE\", \n",
      "    \"name\": \"launch_id\", \n",
      "    \"type\": \"DATE\"\n",
      "  }, \n",
      "  {\n",
      "    \"mode\": \"NULLABLE\", \n",
      "    \"name\": \"camptype\", \n",
      "    \"type\": \"STRING\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$dataset_id\"\n",
    "bq show --format prettyjson --schema $1.campaign_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ReadingData'></a>\n",
    "## 4. Reading from BigQuery\n",
    "### Option #1: BigQuery Magic\n",
    "More information about BigQuery Magic can be found [here](https://cloud.google.com/bigquery/docs/visualize-jupyter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip3 install --upgrade google-cloud-bigquery[pandas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery output_open_rate\n",
    "SELECT sum(opened)/count(*) as aggregate_open_rate\n",
    "FROM `email-propensity-sandbox.emails.sends`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query result is saved to a Pandas dataframe, output_open_rate.   \n",
    "You can now try out data manipulations and visualizations using this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate Open Rate: 18.63%\n"
     ]
    }
   ],
   "source": [
    "print(\"Aggregate Open Rate: {:.2%}\".format(output_open_rate['aggregate_open_rate'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option #2: BigQuery Python Client API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()\n",
    "sql = \"\"\"SELECT sum(opened)/count(*) as aggregate_open_rate\n",
    "    FROM `email-propensity-sandbox.emails.sends`\n",
    "    \"\"\"\n",
    "query_job = client.query(sql) # API request\n",
    "result = query_job.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last line, we're running the query and saving the results to a Pandas Dataframe.   \n",
    "So, any data manipulations or visualizations that you did with the results from BigQuery Magic can also be used for these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate Open Rate: 18.63%\n"
     ]
    }
   ],
   "source": [
    "print(\"Aggregate Open Rate: {:.2%}\".format(output_open_rate['aggregate_open_rate'][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
