{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "## Contents\n",
    "1. [BigQuery ML](#BQML)  \n",
    "    1.1 [Training](#BQML_train)  \n",
    "    1.2 [Evaluation](#BQML_eval)  \n",
    "    1.3 [Prediction](#BQML_pred)\n",
    "2. [AutoML Tables](#AutoMLTables)  \n",
    "    2.1 [AutoML Tables UI](#AutoMLTablesUI)  \n",
    "    2.2 [AutoML Tables API](#AutoMLTablesAPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='BQML'></a>\n",
    "# BigQuery ML (BQML)\n",
    "Reference the [CREATE MODEL syntax](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create) to learn about additional model_options for your BigQuery ML model.  \n",
    "<br/>\n",
    "This is a great option if you are very comfortable with SQL and want to quickly iterate and test models.\n",
    "<br/>\n",
    "BigQuery ML takes care of the following preprocessing steps:\n",
    "- Null imputation\n",
    "- One-hot encoding  \n",
    "<br/>\n",
    "<a id='BQML_train'></a>  \n",
    "\n",
    "## Train BQML Model\n",
    "The below example assumes that you have already loaded a preprocessed table into BigQuery (See `Preprocessing.ipynb` for more information on preprocessing).  \n",
    "If you want to additional preprocessing in BigQuery, just add the transformations to the select statement.  \n",
    "<br>The below code sample will only train a model if a model with the same name does not yet exist. This requirement ensures that we can compare model iterations. If you would like to train a new model, change `CREATE MODEL IF NOT EXISTS` to:\n",
    "- `CREATE OR REPLACE MODEL [existing_model_name]`: if you would like to overwrite an existing model, if it exists\n",
    "- `CREATE MODEL IF NOT EXISTS [new_model_name]`: if you would like to create a new model, not overwriting the old model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "CREATE MODEL IF NOT EXISTS `test_upload.sample_model`\n",
    "OPTIONS(\n",
    "    MODEL_TYPE='logistic_reg',\n",
    "    INPUT_LABEL_COLS = ['opened'],\n",
    "    DATA_SPLIT_METHOD = 'CUSTOM',\n",
    "    DATA_SPLIT_COL = 'eval'\n",
    "    ) AS\n",
    "SELECT * EXCEPT(campaign_send_dt, riid) # Use all columns as features besides key columns (campaign_send_dt and riid)\n",
    "FROM `test_upload.pandas_table`\n",
    "\"\"\"\n",
    "\n",
    "client = bigquery.Client()\n",
    "query_job = client.query(sql) # API request\n",
    "result = query_job.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='BQML_eval'></a>  \n",
    "## Evaluate BQML Model\n",
    "You have multiple options for analyzing a BQML model's evaluation metrics (i.e. precision, recall, etc...).  \n",
    "<br/>\n",
    "As long as you don't overwrite your old BQML models (i.e. by running `CREATE OR REPLACE MODEL...` and not using a new model name), you'll have a collection of old BigQuery models to reference and compare.\n",
    "\n",
    "### Option #1: Via BigQuery UI\n",
    "Evaluation metrics for each of your models can be found in the [BigQuery UI](https://console.cloud.google.com/bigquery) under the Evaluation tab.\n",
    "<br>\n",
    "<img src=\"img/eval_metrics_bqml.png\" title=\"Eval Metrics\"/>   \n",
    "<br>\n",
    "Available metrics include:\n",
    "- ROC AUC\n",
    "- Log loss\n",
    "- Interactive (for different classification thresholds) precision, recall, accuracy, F1 score metrics\n",
    "- Confusion matrix\n",
    "- Precision-recall curve\n",
    "- Precision and Recall vs. Threshold\n",
    "- ROC Curve  \n",
    "  \n",
    "  \n",
    "### Option #2: Via BigQueryML\n",
    "You can also access Evaluation Metrics using BQML queries, as shown in the samples below. More information about using `ML.EVALUATE` can be found [here](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-evaluate).  \n",
    "  \n",
    "You can either use the Python BigQuery API (from this notebook) or the [BigQuery UI](https://console.cloud.google.com/bigquery) to run these queries.\n",
    "\n",
    "If you don't specify a table for `ML.EVALUATE`, the metrics will based on Evaluation data (as specified during model training). If there are more columns in the provided or default table than were used for model training (i.e. key columns), these columns will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   precision    recall  accuracy  f1_score  log_loss   roc_auc\n",
      "0   0.618956  0.279507  0.834747  0.385108  0.423249  0.573457\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "SELECT *\n",
    "FROM ML.EVALUATE(MODEL `test_upload.sample_model`)\n",
    "\"\"\"\n",
    "query_job = client.query(sql) # API request\n",
    "result = query_job.to_dataframe()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "You can also specify a table and/or custom threshold.  \n",
    "  \n",
    "If your source table has different column names and transformations than the table used for training, make sure to apply these transformations and rename the columns before using it to query evaluation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   precision    recall  accuracy  f1_score  log_loss   roc_auc\n",
      "0   0.620039  0.276913  0.834707  0.382845  0.423249  0.573457\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "SELECT *\n",
    "FROM ML.EVALUATE(MODEL `test_upload.sample_model`,\n",
    "    (\n",
    "    SELECT opened,\n",
    "        hist_opens,\n",
    "        hist_sends,\n",
    "        hist_open_rate\n",
    "    FROM `test_upload.pandas_table`\n",
    "    WHERE eval),\n",
    "    STRUCT(0.55 AS threshold))\n",
    "\"\"\"\n",
    "query_job = client.query(sql) # API request\n",
    "result = query_job.to_dataframe()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC Curve\n",
    "`ML.ROC_CURVE` returns evaluation metrics for different classification thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "    SELECT\n",
    "      *\n",
    "    FROM\n",
    "      ML.ROC_CURVE(MODEL `test_upload.sample_model`,\n",
    "        TABLE `test_upload.pandas_table`)\n",
    "\"\"\"\n",
    "query_job = client.query(sql) # API request\n",
    "result = query_job.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>recall</th>\n",
       "      <th>false_positive_rate</th>\n",
       "      <th>true_positives</th>\n",
       "      <th>false_positives</th>\n",
       "      <th>true_negatives</th>\n",
       "      <th>false_negatives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.960219</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>81374</td>\n",
       "      <td>18624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.743504</td>\n",
       "      <td>0.052668</td>\n",
       "      <td>0.002593</td>\n",
       "      <td>981</td>\n",
       "      <td>211</td>\n",
       "      <td>81163</td>\n",
       "      <td>17645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.654836</td>\n",
       "      <td>0.121980</td>\n",
       "      <td>0.009131</td>\n",
       "      <td>2272</td>\n",
       "      <td>743</td>\n",
       "      <td>80631</td>\n",
       "      <td>16354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.553906</td>\n",
       "      <td>0.272791</td>\n",
       "      <td>0.039939</td>\n",
       "      <td>5081</td>\n",
       "      <td>3250</td>\n",
       "      <td>78124</td>\n",
       "      <td>13545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.488284</td>\n",
       "      <td>0.293407</td>\n",
       "      <td>0.045838</td>\n",
       "      <td>5465</td>\n",
       "      <td>3730</td>\n",
       "      <td>77644</td>\n",
       "      <td>13161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.317057</td>\n",
       "      <td>0.348599</td>\n",
       "      <td>0.074078</td>\n",
       "      <td>6493</td>\n",
       "      <td>6028</td>\n",
       "      <td>75346</td>\n",
       "      <td>12133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.239029</td>\n",
       "      <td>0.368034</td>\n",
       "      <td>0.088996</td>\n",
       "      <td>6855</td>\n",
       "      <td>7242</td>\n",
       "      <td>74132</td>\n",
       "      <td>11771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.170302</td>\n",
       "      <td>0.378664</td>\n",
       "      <td>0.099037</td>\n",
       "      <td>7053</td>\n",
       "      <td>8059</td>\n",
       "      <td>73315</td>\n",
       "      <td>11573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.167541</td>\n",
       "      <td>0.811178</td>\n",
       "      <td>0.585199</td>\n",
       "      <td>15109</td>\n",
       "      <td>47620</td>\n",
       "      <td>33754</td>\n",
       "      <td>3517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.110966</td>\n",
       "      <td>0.923977</td>\n",
       "      <td>0.810283</td>\n",
       "      <td>17210</td>\n",
       "      <td>65936</td>\n",
       "      <td>15438</td>\n",
       "      <td>1416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.102016</td>\n",
       "      <td>0.969935</td>\n",
       "      <td>0.919593</td>\n",
       "      <td>18066</td>\n",
       "      <td>74831</td>\n",
       "      <td>6543</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.093711</td>\n",
       "      <td>0.988994</td>\n",
       "      <td>0.969868</td>\n",
       "      <td>18421</td>\n",
       "      <td>78922</td>\n",
       "      <td>2452</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.086018</td>\n",
       "      <td>0.996671</td>\n",
       "      <td>0.989948</td>\n",
       "      <td>18564</td>\n",
       "      <td>80556</td>\n",
       "      <td>818</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.055523</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18626</td>\n",
       "      <td>81374</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    threshold    recall  false_positive_rate  true_positives  false_positives  \\\n",
       "0    0.960219  0.000107             0.000000               2                0   \n",
       "1    0.743504  0.052668             0.002593             981              211   \n",
       "2    0.654836  0.121980             0.009131            2272              743   \n",
       "3    0.553906  0.272791             0.039939            5081             3250   \n",
       "4    0.488284  0.293407             0.045838            5465             3730   \n",
       "5    0.317057  0.348599             0.074078            6493             6028   \n",
       "6    0.239029  0.368034             0.088996            6855             7242   \n",
       "7    0.170302  0.378664             0.099037            7053             8059   \n",
       "8    0.167541  0.811178             0.585199           15109            47620   \n",
       "9    0.110966  0.923977             0.810283           17210            65936   \n",
       "10   0.102016  0.969935             0.919593           18066            74831   \n",
       "11   0.093711  0.988994             0.969868           18421            78922   \n",
       "12   0.086018  0.996671             0.989948           18564            80556   \n",
       "13   0.055523  1.000000             1.000000           18626            81374   \n",
       "\n",
       "    true_negatives  false_negatives  \n",
       "0            81374            18624  \n",
       "1            81163            17645  \n",
       "2            80631            16354  \n",
       "3            78124            13545  \n",
       "4            77644            13161  \n",
       "5            75346            12133  \n",
       "6            74132            11771  \n",
       "7            73315            11573  \n",
       "8            33754             3517  \n",
       "9            15438             1416  \n",
       "10            6543              560  \n",
       "11            2452              205  \n",
       "12             818               62  \n",
       "13               0                0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning probability threshold  \n",
    "We can tune the threshold to achieve a certain recall (then you will live with whatever precision you get). Let’s say that we want to make sure to identify at least 70% of opened emails, i.e. we want a recall of 0.7.   \n",
    "<br/>We can identify this graph by simply using looking at the chart above, referencing the interactive plots in the BigQuery UI, or by using the below query to identify the given threshold (as explained [here](https://towardsdatascience.com/how-to-tune-a-bigquery-ml-classification-model-to-achieve-a-desired-precision-or-recall-e4d40b93016a))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   threshold    recall  false_positive_rate  from_desired_recall\n",
      "0   0.167541  0.810714             0.586374             0.110714\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "    WITH roc AS (\n",
    "        SELECT\n",
    "          *\n",
    "        FROM\n",
    "          ML.ROC_CURVE(MODEL `test_upload.sample_model`,\n",
    "            (SELECT opened,\n",
    "                hist_opens,\n",
    "                hist_sends,\n",
    "                hist_open_rate\n",
    "            FROM `test_upload.pandas_table`\n",
    "            WHERE eval = False)\n",
    "            ))\n",
    "    SELECT\n",
    "        threshold,\n",
    "        recall, false_positive_rate,\n",
    "        ABS(recall - 0.7) AS from_desired_recall\n",
    "    FROM roc\n",
    "    ORDER BY from_desired_recall ASC\n",
    "    LIMIT 1    \n",
    "\"\"\"\n",
    "query_job = client.query(sql) # API request\n",
    "result = query_job.to_dataframe()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "More information about BQML Confusion Matrices can be found [here](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expected_label</th>\n",
       "      <th>_0</th>\n",
       "      <th>_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>19575</td>\n",
       "      <td>785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3345</td>\n",
       "      <td>1281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   expected_label     _0    _1\n",
       "0               0  19575   785\n",
       "1               1   3345  1281"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  ML.CONFUSION_MATRIX(MODEL `test_upload.sample_model`,\n",
    "  (\n",
    "    SELECT *\n",
    "    FROM `test_upload.pandas_table`\n",
    "    WHERE eval),\n",
    "    STRUCT(0.55 AS threshold)\n",
    "    )\n",
    "\"\"\"\n",
    "query_job = client.query(sql) # API request\n",
    "result = query_job.to_dataframe()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Info\n",
    "`ML.FEATURE_INFO`, as explained [here](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-feature), returns information about input features used to train the model including:\n",
    "- input: name of the column\n",
    "- min: sample minimum (NULL if categorical)\n",
    "- max: sample maximum (NULL if categorical)\n",
    "- mean: sample average (NULL if categorical)\n",
    "- stddev: sample standard deviation (NULL if categorical)\n",
    "- categorical_count: number of categories (NULL if not categorical)\n",
    "- null_count - number of NULLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>stddev</th>\n",
       "      <th>category_count</th>\n",
       "      <th>null_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hist_opens</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.228517</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.633862</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hist_sends</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.008878</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.283603</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hist_open_rate</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.215606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.370378</td>\n",
       "      <td>None</td>\n",
       "      <td>35713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            input  min   max      mean  median    stddev category_count  \\\n",
       "0      hist_opens  0.0   8.0  0.228517     0.0  0.633862           None   \n",
       "1      hist_sends  0.0  10.0  1.008878     1.0  1.283603           None   \n",
       "2  hist_open_rate  0.0   1.0  0.215606     0.0  0.370378           None   \n",
       "\n",
       "   null_count  \n",
       "0           0  \n",
       "1           0  \n",
       "2       35713  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "    SELECT\n",
    "      *\n",
    "    FROM\n",
    "      ML.FEATURE_INFO(MODEL `test_upload.sample_model`)\n",
    "\"\"\"\n",
    "\n",
    "query_job = client.query(sql) # API request\n",
    "result = query_job.to_dataframe()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights\n",
    "The `ML.WEIGHTS` function allows you to see the underlying weights used by a model during prediction, as explained [here](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-weights).  \n",
    "  \n",
    "Our example does not include categorical columns. However, if we want to look at weights for categorical (one-hot encoded) features, use the following query:\n",
    "```\n",
    "SELECT\n",
    "  category,\n",
    "  weight\n",
    "FROM\n",
    "  UNNEST((\n",
    "    SELECT\n",
    "      category_weights\n",
    "    FROM\n",
    "      ML.WEIGHTS(MODEL `[dataset_id].[model_name]`)\n",
    "    WHERE\n",
    "      processed_input = '[categorical_column]'))\n",
    "```  \n",
    "It's also very simple to look at the weights of numeric or boolean features, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_input</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hist_opens</td>\n",
       "      <td>0.518016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hist_sends</td>\n",
       "      <td>-0.094116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hist_open_rate</td>\n",
       "      <td>1.779359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__INTERCEPT__</td>\n",
       "      <td>-1.986794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  processed_input    weight\n",
       "0      hist_opens  0.518016\n",
       "1      hist_sends -0.094116\n",
       "2  hist_open_rate  1.779359\n",
       "3   __INTERCEPT__ -1.986794"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "    SELECT\n",
    "      processed_input, weight\n",
    "    FROM\n",
    "      ML.WEIGHTS(MODEL `test_upload.sample_model`)\n",
    "\"\"\"\n",
    "\n",
    "query_job = client.query(sql) # API request\n",
    "result = query_job.to_dataframe()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='BQML_pred'></a>  \n",
    "## Predictions using BQML Model\n",
    "The `ML.PREDICT` function can be used to predict outcomes using the model, as explained [here](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-predict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_opened</th>\n",
       "      <th>predicted_opened_probs</th>\n",
       "      <th>riid</th>\n",
       "      <th>campaign_send_dt</th>\n",
       "      <th>opened</th>\n",
       "      <th>hist_opens</th>\n",
       "      <th>hist_sends</th>\n",
       "      <th>hist_open_rate</th>\n",
       "      <th>eval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'prob': 0.1675412616617039, 'label': 1}, {'p...</td>\n",
       "      <td>737847182</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'prob': 0.1675412616617039, 'label': 1}, {'p...</td>\n",
       "      <td>566134962</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'prob': 0.1675412616617039, 'label': 1}, {'p...</td>\n",
       "      <td>849236702</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'prob': 0.1675412616617039, 'label': 1}, {'p...</td>\n",
       "      <td>825551142</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'prob': 0.1675412616617039, 'label': 1}, {'p...</td>\n",
       "      <td>825759702</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   predicted_opened                             predicted_opened_probs  \\\n",
       "0                 0  [{'prob': 0.1675412616617039, 'label': 1}, {'p...   \n",
       "1                 0  [{'prob': 0.1675412616617039, 'label': 1}, {'p...   \n",
       "2                 0  [{'prob': 0.1675412616617039, 'label': 1}, {'p...   \n",
       "3                 0  [{'prob': 0.1675412616617039, 'label': 1}, {'p...   \n",
       "4                 0  [{'prob': 0.1675412616617039, 'label': 1}, {'p...   \n",
       "\n",
       "        riid campaign_send_dt  opened  hist_opens  hist_sends  hist_open_rate  \\\n",
       "0  737847182       2018-01-01       0           0           0             NaN   \n",
       "1  566134962       2018-01-01       0           0           0             NaN   \n",
       "2  849236702       2018-01-01       0           0           0             NaN   \n",
       "3  825551142       2018-01-01       0           0           0             NaN   \n",
       "4  825759702       2018-01-01       0           0           0             NaN   \n",
       "\n",
       "   eval  \n",
       "0  True  \n",
       "1  True  \n",
       "2  True  \n",
       "3  True  \n",
       "4  True  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "    SELECT\n",
    "      *\n",
    "    FROM\n",
    "      ML.PREDICT(MODEL `test_upload.sample_model`,\n",
    "      (\n",
    "        SELECT *\n",
    "        FROM `test_upload.pandas_table`\n",
    "        WHERE eval),\n",
    "        STRUCT(0.55 AS threshold)\n",
    "        )\n",
    "\"\"\"\n",
    "\n",
    "query_job = client.query(sql) # API request\n",
    "result = query_job.to_dataframe()\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted probabilities for each class are stored in a nested array. We can use BigQuery's `UNNEST` function to find the probabilities of an opened email.  \n",
    "\n",
    "Notice that the name of the prediction column (`predicted_opened`) is formatted `predicted_[name_of_label_column]` and the column containing the nested probabilities (`predicted_opened_probs`) is formatted `predicted_[name_of_label_column]_probs`. You will need to replace the `opened` with the name of your label column in the code samples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>riid</th>\n",
       "      <th>campaign_send_dt</th>\n",
       "      <th>predicted_opened</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>737847182</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0.167541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>566134962</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0.167541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>849236702</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0.167541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>825551142</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0.167541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>825759702</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0.167541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        riid campaign_send_dt  predicted_opened      prob\n",
       "0  737847182       2018-01-01                 0  0.167541\n",
       "1  566134962       2018-01-01                 0  0.167541\n",
       "2  849236702       2018-01-01                 0  0.167541\n",
       "3  825551142       2018-01-01                 0  0.167541\n",
       "4  825759702       2018-01-01                 0  0.167541"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = \"\"\"WITH results  AS (\n",
    "      SELECT\n",
    "        *\n",
    "      FROM\n",
    "        ML.PREDICT(MODEL `test_upload.sample_model`,\n",
    "        (\n",
    "          SELECT *\n",
    "          FROM `test_upload.pandas_table`\n",
    "          WHERE eval),\n",
    "          STRUCT(0.55 AS threshold)\n",
    "          ))\n",
    "    SELECT riid,\n",
    "        campaign_send_dt,\n",
    "        predicted_opened, # Replace with predict_[name_of_label_column]\n",
    "        probs.prob\n",
    "    FROM results, UNNEST(predicted_opened_probs) as probs # Replace table in UNNEST(...) with predict_[name_of_label_column]_probs\n",
    "    WHERE probs.label = 1\"\"\"\n",
    "\n",
    "query_job = client.query(sql) # API request\n",
    "result = query_job.to_dataframe()\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='AutoMLTables'></a>\n",
    "# AutoML Tables\n",
    "AutoML Tables trains supervised learning models on structured data using neural network architecture search.  \n",
    "  \n",
    "Keep in mind that you can access the same datasets and models from both the AutoML Tables UI and the AutoML Tables API.  \n",
    "  \n",
    "Like BQML, AutoML tables will preserve your model versions so that you can compare evaluation metrics and predictions.  \n",
    "<a id='AutoMLTablesUI'></a>\n",
    "## Option #1: AutoML Tables UI\n",
    "You can use the [AutoML Tables UI](https://console.cloud.google.com/automl-tables) to import data.\n",
    "1. Select \"Create Dataset\" and name dataset.\n",
    "2. Select \"Import data from BigQuery\" and input the following values  \n",
    "  -  BigQuery Project ID: levis-data-science-challenge\n",
    "  -  BigQuery Dataset ID: [BigQuery_dataset_ID]\n",
    "  -  BigQuery Table or View ID: [table_name_of_preprocessed_BQ_table]\n",
    "3. Look at Schema tab (check Data Type and Nullability) and select a target column\n",
    "  -  If you don't see the \"Select a target\" column, make sure your window is wide enough. \n",
    "  -  For this example dataset, select `opened` as the target column and press \"Continue\"\n",
    "4. Look at Analyze tab (check for anomalies in the data)\n",
    "5. Look at Train tab to start training job  \n",
    "  -  Input numeric (1-72) budget\n",
    "  -  Deselect key columns using \"Input feature selection\" (i.e. riid and campaign_send_dt)\n",
    "  -  Select \"Advanced options\" to use a different optimization objective (i.e. Log loss) and/or turn off Early stopping if desired.\n",
    "  - Select \"Train\"  \n",
    "  \n",
    "You will get an email when training completes. \n",
    "More information about training via the AutoML Table UI can be found [here](https://cloud.google.com/automl-tables/docs/quickstart).  \n",
    "  \n",
    "<a id='AutoMLTablesAPI'></a>\n",
    "## Option #2: AutoML Tables API  \n",
    "Follow the below example to use the AuttoML Tables API. Reference the markdown cell above the code to ensure that your code is updated to your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip3 install google-cloud-automl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import automl_v1beta1\n",
    "from google.cloud import bigquery\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `project_id` to your GCP project ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = automl_v1beta1.AutoMlClient()\n",
    "prediction_client = automl_v1beta1.PredictionServiceClient()\n",
    "project_id = 'email-propensity-sandbox' # replace with your project ID\n",
    "location = 'us-central1'\n",
    "location_path = client.location_path(project_id, location)\n",
    "location_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `dataset_display_name` to your desired dataset name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_display_name = 'colab_trial11' # replace with your desired dataset name\n",
    "create_dataset_response = client.create_dataset(\n",
    "    location_path,\n",
    "    {\n",
    "        'display_name': dataset_display_name,\n",
    "        'tables_dataset_metadata': {}\n",
    "    }\n",
    ")\n",
    "dataset_name = create_dataset_response.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `dataset_bq_input_uri` to `bq://[project_id].[dataset_id].[bq_table]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_bq_input_uri = 'bq://email-propensity-sandbox.test_upload.pandas_table'\n",
    "# Define input configuration.\n",
    "input_config = {\n",
    "    'bigquery_source': {\n",
    "        'input_uri': dataset_bq_input_uri\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell will continue running until the dataset is done uploading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset import operation: name: \"projects/1030417721972/locations/us-central1/operations/TBL9051439204596187136\"\n",
      "metadata {\n",
      "  type_url: \"type.googleapis.com/google.cloud.automl.v1beta1.OperationMetadata\"\n",
      "  value: \"\\032\\014\\010\\347\\236\\263\\352\\005\\020\\370\\355\\330\\324\\003\\\"\\014\\010\\347\\236\\263\\352\\005\\020\\370\\355\\330\\324\\003z\\000\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import_data_response = client.import_data(dataset_name, input_config)\n",
    "print('Dataset import operation: {}'.format(import_data_response.operation))\n",
    "# Wait until import is done.\n",
    "import_data_result = import_data_response.result()\n",
    "import_data_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review the table specs\n",
    "Run the following command to see table specs (i.e. row count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('eval', 'CATEGORY'),\n",
       " ('hist_sends', 'CATEGORY'),\n",
       " ('campaign_send_dt', 'TIMESTAMP'),\n",
       " ('riid', 'FLOAT64'),\n",
       " ('hist_open_rate', 'FLOAT64'),\n",
       " ('hist_opens', 'CATEGORY'),\n",
       " ('opened', 'CATEGORY')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import google.cloud.automl_v1beta1.proto.data_types_pb2 as data_types\n",
    "\n",
    "# List table specs\n",
    "list_table_specs_response = client.list_table_specs(dataset_name)\n",
    "table_specs = [s for s in list_table_specs_response]\n",
    "# List column specs\n",
    "table_spec_name = table_specs[0].name\n",
    "list_column_specs_response = client.list_column_specs(table_spec_name)\n",
    "column_specs = {s.display_name: s for s in list_column_specs_response}\n",
    "[(x, data_types.TypeCode.Name(\n",
    "  column_specs[x].data_type.type_code)) for x in column_specs.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update dataset (labels and data type)\n",
    "AutoML Tables automatically detects your data column type.  \n",
    "  \n",
    "Depending on the type of your label column, AutoML Tables chooses to run a classification or regression model.  \n",
    "  \n",
    "Set `label_column_name` to the name of your label column.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label column ID: 532168025890095104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "name: \"projects/1030417721972/locations/us-central1/datasets/TBL1257058449896767488\"\n",
       "display_name: \"colab_trial11\"\n",
       "create_time {\n",
       "  seconds: 1565314918\n",
       "  nanos: 298986000\n",
       "}\n",
       "etag: \"AB3BwFo49585RFFdUZj2yP0L7oUHTQvJnlMnuItBtoQ1_PazB0cvsrg_j-ehZUqJRefx\"\n",
       "example_count: 100000\n",
       "tables_dataset_metadata {\n",
       "  primary_table_spec_id: \"3736914567368802304\"\n",
       "  target_column_spec_id: \"532168025890095104\"\n",
       "  stats_update_time {\n",
       "    seconds: 1565315026\n",
       "    nanos: 184000000\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_column_name = 'opened' # replace with name of label column\n",
    "label_column_spec = column_specs[label_column_name]\n",
    "label_column_id = label_column_spec.name.rsplit('/', 1)[-1]\n",
    "print('Label column ID: {}'.format(label_column_id))\n",
    "\n",
    "update_dataset_dict = {\n",
    "    'name': dataset_name,\n",
    "    'tables_dataset_metadata': {\n",
    "        'target_column_spec_id': label_column_id\n",
    "    }\n",
    "}\n",
    "update_dataset_response = client.update_dataset(update_dataset_dict)\n",
    "update_dataset_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If any of your features were detected as the wrong data type, use the below code to change the data type. Valid data_type options can be found [here](https://cloud.google.com/automl-tables/docs/reference/rpc/google.cloud.automl.v1beta1#google.cloud.automl.v1beta1.TypeCode).  \n",
    "  \n",
    "Set `column_to_category` to the name of the relevant column and `type_code` to a string representation of your desired data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('hist_sends', type_code: FLOAT64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_to_category = 'hist_sends'\n",
    "\n",
    "update_column_spec_dict = {\n",
    "    \"name\": column_specs[column_to_category].name,\n",
    "    \"data_type\": {\n",
    "        \"type_code\": \"FLOAT64\"\n",
    "    }\n",
    "}\n",
    "update_column_response = client.update_column_spec(update_column_spec_dict)\n",
    "update_column_response.display_name , update_column_response.data_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('hist_opens', type_code: FLOAT64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_to_category = 'hist_opens'\n",
    "\n",
    "update_column_spec_dict = {\n",
    "    \"name\": column_specs[column_to_category].name,\n",
    "    \"data_type\": {\n",
    "        \"type_code\": \"FLOAT64\"\n",
    "    }\n",
    "}\n",
    "update_column_response = client.update_column_spec(update_column_spec_dict)\n",
    "update_column_response.display_name , update_column_response.data_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the table schema again to see the new values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('eval', 'CATEGORY'),\n",
       " ('hist_sends', 'FLOAT64'),\n",
       " ('campaign_send_dt', 'TIMESTAMP'),\n",
       " ('riid', 'FLOAT64'),\n",
       " ('hist_open_rate', 'FLOAT64'),\n",
       " ('hist_opens', 'FLOAT64'),\n",
       " ('opened', 'CATEGORY')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List table specs\n",
    "list_table_specs_response = client.list_table_specs(dataset_name)\n",
    "table_specs = [s for s in list_table_specs_response]\n",
    "# List column specs\n",
    "table_spec_name = table_specs[0].name\n",
    "list_column_specs_response = client.list_column_specs(table_spec_name)\n",
    "column_specs = {s.display_name: s for s in list_column_specs_response}\n",
    "[(x, data_types.TypeCode.Name(\n",
    "  column_specs[x].data_type.type_code)) for x in column_specs.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "Set `model_display_name` to your desired name for the model.  \n",
    "Set `model_train_hours` to a training budget which should be an integer between 1 and 72.  \n",
    "Set `model_optimization_objective` to your desired optimization objective. Options for binary classification are \"MAXIMIZE_AU_ROC\" (default), \"MINIMIZE_LOG_LOSS\", or \"MAXIMIZE_AU_PRC\".  \n",
    "Set `columns_to_ignore` to your key columns, or any other columns that should not be used as features.  \n",
    "  \n",
    "You can stop the cell and the model will continue training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset import operation: name: \"projects/1030417721972/locations/us-central1/operations/TBL5414219555541090304\"\n",
      "metadata {\n",
      "  type_url: \"type.googleapis.com/google.cloud.automl.v1beta1.OperationMetadata\"\n",
      "  value: \"\\032\\014\\010\\301\\251\\263\\352\\005\\020\\320\\260\\265\\270\\002\\\"\\014\\010\\301\\251\\263\\352\\005\\020\\320\\260\\265\\270\\002R\\000\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-ed6129f931ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Dataset import operation: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_model_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Wait until model training is done.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mcreate_model_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mcreate_model_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/google/api_core/future/polling.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mthe\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreached\u001b[0m \u001b[0mbefore\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moperation\u001b[0m \u001b[0mcompletes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \"\"\"\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blocking_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/google/api_core/future/polling.py\u001b[0m in \u001b[0;36m_blocking_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mretry_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_done_or_raise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRetryError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             raise concurrent.futures.TimeoutError(\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0msleep_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m                 \u001b[0mon_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m             )\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, deadline, on_error)\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0;34m\"Retrying due to {}, sleeping {:.1f}s ...\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_exc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msleep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         )\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sleep generator stopped yielding sleep values.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_display_name = 'example_model' #@param {type:'string'}\n",
    "model_train_hours = 12 #@param {type:'integer'}\n",
    "model_optimization_objective = 'MINIMIZE_LOG_LOSS' #@param {type:'string'}\n",
    "columns_to_ignore = ['riid', 'campaign_send_dt'] #@param {type:'string'}\n",
    "\n",
    "# Create list of features to use\n",
    "feat_list = list(column_specs.keys())\n",
    "feat_list.remove(label_column_name)\n",
    "for c in columns_to_ignore:\n",
    "    feat_list.remove(c)\n",
    "\n",
    "model_dict = {\n",
    "    'display_name': model_display_name,\n",
    "    'dataset_id': dataset_name.rsplit('/', 1)[-1],\n",
    "    'tables_model_metadata': {\n",
    "      'train_budget_milli_node_hours':model_train_hours * 1000,\n",
    "      'optimization_objective': model_optimization_objective,\n",
    "      'target_column_spec': column_specs[label_column_name],\n",
    "      'input_feature_column_specs': [\n",
    "            column_specs[x] for x in feat_list]}\n",
    "    }\n",
    "    \n",
    "create_model_response = client.create_model(location_path, model_dict)\n",
    "print('Dataset import operation: {}'.format(create_model_response.operation))\n",
    "# Wait until model training is done.\n",
    "create_model_result = create_model_response.result()\n",
    "model_name = create_model_result.name\n",
    "create_model_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you stopped the above cell, run the following command to check if model training is complete. It will return `True` if the model is done training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_model_response.done()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your notebook has timed out, use `client.list_models(location_path)` to check whether your model has finished training. You will also get an email when it's done training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'projects/email-propensity/locations/us-central1/models/colab_trial11'\n",
    "model = client.get_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_model_result = create_model_response.result()\n",
    "model_name = create_model_result.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your notebook has timed out run the following code to retrieve your model name.\n",
    "```\n",
    "model_name = 'projects/email-propensity/locations/us-central1/models/colab_trial11'\n",
    "```\n",
    "Set `model_name` to `projects/<project_id>/locations/<location>/models/<model_id>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = client.get_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics= [x for x in client.list_model_evaluations(model_name)][-1]\n",
    "metrics.regression_evaluation_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = client.get_model(model_name)\n",
    "feat_list = [(x.feature_importance, x.column_display_name) for x in model.tables_model_metadata.tables_model_column_info]\n",
    "feat_list.sort(reverse=True)\n",
    "feat_list[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Predictions  \n",
    "More information about predictions using the AutoML Tables API can be found [here](https://cloud.google.com/automl-tables/docs/predict-batch).  \n",
    "Set `batch_predict_bq_input_uri` to the BQ URI for your input table (to make predictions on).  \n",
    "Set `batch_predict_bq_output` to `bq://<project_id>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_predict_bq_input_uri = 'bq://email-propensity-sandbox.test_upload.pandas_table'\n",
    "batch_predict_bq_output = 'bq://email-propensity-sandbox'\n",
    "# Define input source.\n",
    "batch_prediction_input_source = {\n",
    "  'bigquery_source': {\n",
    "    'input_uri': batch_predict_bq_input_uri\n",
    "  }\n",
    "}\n",
    "# Define output target.\n",
    "batch_prediction_output_target = {\n",
    "    'bigquery_destination': {\n",
    "      'output_uri': batch_predict_bq_output\n",
    "    }\n",
    "}\n",
    "batch_predict_response = prediction_client.batch_predict(\n",
    "    model_name, batch_prediction_input_source, batch_prediction_output_target)\n",
    "print('Batch prediction operation: {}'.format(batch_predict_response.operation))\n",
    "# Wait until batch prediction is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function will return `True` when predictions are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_predict_response.done()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your predictions are stored in a BigQuery table named `<model_id>.predictions`. Predictions are stored in the column `predicted_<target_column>`, which is an Array  \n",
    "You can query this table (using the BQ UI or BQ API) to see predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "    SELECT predicted_opened[OFFSET(0)].tables AS value_1,\n",
    "        predicted_opened[OFFSET(1)].tables AS value_2\n",
    "    FROM colab_trial11.predictions\n",
    "\"\"\"\n",
    "query_job = client.query(sql) # API request\n",
    "result = query_job.to_dataframe()\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
